---
title: Hawaiâ€˜i Climate Smart Agriculture Database Data Pipeline
author: MK Lau
---


<!-- Rendering rmarkdown Notebook -->
```{r render, eval = FALSE, echo = FALSE, message=FALSE, warnings=FALSE}

## Run the following to render the notebook and generate the database
library(pacman)
p_load(rmarkdown, tictoc, curl, gargle)

## Setup google auth
token <- token_fetch(scopes = "https://www.googleapis.com/auth/spreadsheets")
googlesheets4::gs4_auth(token = token)

## Render
tic()
rmarkdown::render("hi-csa-db.Rmd", 
                  output_format = c("html_document"))
toc()

```

# Context

The City and County of Honolulu's Office of Climate Resilience has
contracted OACA to produce a Climate Smart Agriculture database
intended to inform food system professionals and policy makers on the
potential ecosystem service impacts of CSA activities. The consultant
(MK Lau) has been sub-contracted by OACA to complete the deliverables
listed below in partial fulfillment of the larger contracted
deliverable to HC&C.


# Database Construction

- The goal is to construct a database of resources to support climate
  smart agricultural practices, using the USDA NRCS climate smart mitigation
  strategies as a framework. 
- Via the data pipeline described in this document, data are ingested
  into the database starting with hand-extracted data gathered by
  Lucas McKinnon and Jackson Hart and then using a webcrawling to
  gather resources from existing websites, including the NRCS, NIFA,
  AMS, and ATTRA.
- A structured relational database in produced and saved to the main
  directory. 

# Ingest

```{r deps, echo = FALSE}

library(pacman)
p_load(RColorBrewer, ggplot2, rvest, Rcrawler, readtext, 
       knitr, googlesheets4, dplyr, igraph, tictoc)

db_merge <- function(x, y){
    cn.x <- colnames(x)
    cn.y <- colnames(y)
    cn.xy <- cn.y[!(cn.y %in% cn.x)]
    xy <- data.frame(array(dim = c(nrow(x), length(cn.xy))))
    xy <- data.frame(x, xy)
    colnames(xy) <- c(colnames(x), cn.xy)
    xy <- xy[ ,order(colnames(xy))]
    cn.yx <- cn.x[!(cn.x %in% cn.y)]
    yx <- data.frame(array(dim = c(nrow(y), length(cn.yx))))
    yx <- data.frame(y, yx)
    colnames(yx) <- c(colnames(y), cn.yx)
    yx <- yx[ ,order(colnames(yx))]
    out <- rbind(xy, yx)
    return(out)
}

detect.csa <- function(x, practice){
    x <- gsub('[[:punct:] ]+',' ', x)
    x <- strsplit(x, " ")[[1]]
    match.prac <- logical()
    for (i in seq_along(practice[, 1])){
        match.prac[i] <- any(grepl(practice[i, 1], x, ign = TRUE))
    }
    if (any(match.prac)){
        out <- practice[match.prac, 2]        
    }else{
        out <- NA
    }
    return(out)
}

```

## Webcrawl 

There are currently two webcrawl directories for NRCS. The first,
"nrcs.usda.gov-292243", contains a very deep crawl with max depth
= 4. The second, "nrcs.usda.gov-301855", is a shallower crawl with a
max depth of 1, which is currenlty being used to compile data for the
database. 

```{r crawl-nrcs, cache = FALSE, eval = TRUE, results = "hide"}

nrcs.url <- paste0("https://www.nrcs.usda.gov/conservation-basics/", 
                   "natural-resource-concerns/climate/climate-smart-mitigation-activities")

if ("nrcs.rds" %in% dir("data")){

    nrcs <- readRDS(file = "./data/nrcs.rds")

}else{

    Rcrawler(Website = nrcs.url,
             no_cores = 4, no_conn = 4, 
             NetworkData = TRUE,
             NetwExtLinks = TRUE, 
             ExtractXpathPat = "//*/a/@href",
             RequestsDelay = 0,01,
             ManyPerPattern = TRUE, MaxDepth = 1, 
             saveOnDisk = FALSE
             )
    nrcs <- list()
    nrcs[[1]] <- INDEX
    nrcs[[2]] <- DATA
    nrcs[[3]] <- list(NetwIndex, NetwEdges)
    names(nrcs)[1] <- "INDEX"
    names(nrcs)[2] <- "DATA"
    names(nrcs)[3] <- "network"
    names(nrcs[[3]])[1] <- "NetwIndex"
    names(nrcs[[3]])[2] <- "NetwEdges"
    saveRDS(nrcs, file = "data/nrcs.rds", compress = TRUE)

}


```

The following code scrapes key information from the NRCS Climate
Mitigation Strategies webpage. It is then exported to the
`data/hi-csa-es.db`.


```{r nrcs-scrape, eval = TRUE, echo = TRUE, results = "hide"}

## From the 
## 1. Get a list of mitigation categories
## 2. Get a list of practices within each mitigation
## 3. Get URL links to resources
## From the remaining search,
## 1. Get a list of other resources


nrcs.csm <- rvest::read_html(url(nrcs.url))

h2 <- nrcs.csm %>% html_elements("h2")
h3 <- nrcs.csm %>% html_elements("h3")
h4 <- nrcs.csm %>% html_elements("h4")
p <- nrcs.csm %>% html_elements("p")
a <- nrcs.csm %>% html_elements("a")
div <- nrcs.csm %>% html_elements("div")

nrcs.csm %>% html_elements("h3")
nrcs.csm %>% html_elements("h4")
nrcs.csm %>% html_elements("body")
nrcs.csm %>% html_elements(".title")

headers <- nrcs.csm %>% html_elements("h3, h4, p")

# Load the web page
webpage <- read_html(url(nrcs.url))

# Extract all headers (h3, h4) and paragraphs (p)
elements <- webpage %>% html_elements("h3, h4, p")

# Initialize lists to store the associations
result <- list()
current_h3 <- NULL
current_h4 <- NULL

# Loop through each element and determine its tag type
for (element in elements) {
  # Get the text, tag name, and anchor tags within the current element
  element_text = element %>% html_text(trim = TRUE)
  tag_name = element %>% html_name()

  # Check if it's an h3 header
  if (tag_name == "h3") {
    # If it's an h3, update the current context
    current_h3 = element_text
    result[[current_h3]] = list("h4" = list(), "p" = list())
  } else if (tag_name == "h4" && !is.null(current_h3)) {
    # If it's an h4, update the current context
    current_h4 = element_text
    result[[current_h3]]$h4[[current_h4]] = list("p" = list())
  } else if (tag_name == "p") {
    # If it's a paragraph, add it to the corresponding context
    paragraph_data = list("text" = element_text, "links" = list())

    # Check for any anchor tags (links) within the paragraph
    anchors = element %>% html_elements("a")
    
    if (length(anchors) > 0) {
      paragraph_data$links = lapply(anchors, function(anchor) {
        list("text" = anchor %>% html_text(trim = TRUE), "href" = anchor %>% html_attr("href"))
      })
    }
    
    if (!is.null(current_h4) && !is.null(current_h3)) {
      result[[current_h3]]$h4[[current_h4]]$p <- append(result[[current_h3]]$h4[[current_h4]]$p, list(paragraph_data))
    } else if (!is.null(current_h3)) {
      result[[current_h3]]$p <- append(result[[current_h3]]$p, list(paragraph_data))
    }
  }
}


# Define a function to convert the list into a data frame
list_to_dataframe <- function(result) {
  # Initialize an empty data frame
  data <- data.frame(H3 = character(), H4 = character(), p = character(), a = character(), stringsAsFactors = FALSE)

  # Iterate through the result list to build the data frame
  for (h3_name in names(result)) {
    h3_entry <- result[[h3_name]]
    
    # Extract paragraphs for h3-level
    if ("p" %in% names(h3_entry)) {
      for (p_item in h3_entry$p) {
        # Check if the paragraph contains links
        if ("links" %in% names(p_item)) {
          for (link in p_item$links) {
            new_row <- data.frame(
              H3 = h3_name,
              H4 = NA,
              p = p_item$text,
              a = link$href,
              stringsAsFactors = FALSE
            )
            data <- bind_rows(data, new_row)
          }
        }
      }
    }

    # Extract h4-level entries
    if ("h4" %in% names(h3_entry)) {
      for (h4_name in names(h3_entry$h4)) {
        h4_entry <- h3_entry$h4[[h4_name]]
        
        # Extract paragraphs and links at h4-level
        if ("p" %in% names(h4_entry)) {
          for (p_item in h4_entry$p) {
            if ("links" %in% names(p_item)) {
              for (link in p_item$links) {
                new_row <- data.frame(
                  H3 = h3_name,
                  H4 = h4_name,
                  p = p_item$text,
                  a = link$href,
                  stringsAsFactors = FALSE
                )
                data <- bind_rows(data, new_row)
              }
            }
          }
        }
      }
    }
  }

  return(data)
}

## Use the function to convert the list into a data frame
nrcs.db <- list_to_dataframe(result)
## Add the full path for the nrcs urls
for (i in seq_along(nrcs.db[, "a"])){
    if (!grepl("http", nrcs.db[i, "a"])){
        nrcs.db[i, "a"] <- paste0("https://www.nrcs.usda.gov", nrcs.db[i, "a"])
    }else{}
}


## Prep for export
colnames(nrcs.db) <- c("Mitigation", "Practice", "Description", "Resource")

```


```{r crawl-attra, cache = FALSE, eval = TRUE, results = "hide"}

attra.url <- "https://attra.ncat.org/publication/climate-beneficial-practices/"

if ("attra.rds" %in% dir("data")){

    attra <- readRDS(file = "./data/attra.rds")

}else{

    Rcrawler(Website = attra.url,
             no_cores = 4, no_conn = 4, 
             NetworkData = TRUE,
             NetwExtLinks = TRUE, 
             ExtractXpathPat = "//*/a/@href",
             RequestsDelay = 0,01,
             ManyPerPattern = TRUE, MaxDepth = 2, 
             saveOnDisk = FALSE
             )
    attra <- list()
    attra[[1]] <- INDEX
    attra[[2]] <- DATA
    attra[[3]] <- list(NetwIndex, NetwEdges)
    names(attra)[1] <- "INDEX"
    names(attra)[2] <- "DATA"
    names(attra)[3] <- "network"
    names(attra[[3]])[1] <- "NetwIndex"
    names(attra[[3]])[2] <- "NetwEdges"
    saveRDS(attra, file = "./data/attra.rds", compress = FALSE)
}

```

```{r crawl-nifa, eval = TRUE, echo = TRUE, results = "hide"}

nifa.url <- "https://www.nifa.usda.gov/grants"

if ("nifa.rds" %in% dir("data")){

    rds <- readRDS(file = "./data/nifa.rds")

}else{

    Rcrawler(Website = nifa.url,
             no_cores = 4, no_conn = 4, 
             NetworkData = TRUE,
             NetwExtLinks = TRUE, 
             ExtractXpathPat = "//*/a/@href",
             RequestsDelay = 0,01,
             ManyPerPattern = TRUE, MaxDepth = 1, 
             saveOnDisk = FALSE
             )
    nifa <- list()
    nifa[[1]] <- INDEX
    nifa[[2]] <- DATA
    nifa[[3]] <- list(NetwIndex, NetwEdges)
    names(nifa)[1] <- "INDEX"
    names(nifa)[2] <- "DATA"
    names(nifa)[3] <- "network"
    names(nifa[[3]])[1] <- "NetwIndex"
    names(nifa[[3]])[2] <- "NetwEdges"
    saveRDS(nifa, file = "./data/nifa.rds", compress = FALSE)
}

```



```{r crawl-ams, eval = TRUE, echo = TRUE, results = "hide"}

ams.url <- "https://www.ams.usda.gov/services/grants"

if ("ams.rds" %in% dir("data")){

    ams <- readRDS(file = "./data/ams.rds")

}else{

    Rcrawler(Website = ams.url,
             no_cores = 4, no_conn = 4, 
             NetworkData = TRUE,
             NetwExtLinks = TRUE, 
             ExtractXpathPat = "//*/a/@href",
             RequestsDelay = 0,01,
             ManyPerPattern = TRUE, MaxDepth = 1, 
             saveOnDisk = FALSE
             )
    ams <- list()
    ams[[1]] <- INDEX
    ams[[2]] <- DATA
    ams[[3]] <- list(NetwIndex, NetwEdges)
    names(ams)[1] <- "INDEX"
    names(ams)[2] <- "DATA"
    names(ams)[3] <- "network"
    names(ams[[3]])[1] <- "NetwIndex"
    names(ams[[3]])[2] <- "NetwEdges"
    saveRDS(ams, file = "./data/ams.rds", compress = FALSE)

}


```


## Other data to integrate

- https://gofarmhawaii.org/farmer-resources-2/ 
- https://www.fsa.usda.gov/programs-and-services/farm-loan-programs/
- https://www.fns.usda.gov/fm/grant-opportunities
- https://www.rd.usda.gov/


```{r nrcs-cw, eval = FALSE, echo = TRUE}

nrc.url <- nrc.wc[nrc.wc[, "Level"] == 2, "Url"]
nrc.url <- nrc.url[grepl("natural-resource-concerns/", nrc.url)]
nrc.cat <- strsplit(nrc.url, "natural-resource-concerns/")
nrc.url <- nrc.url[lapply(nrc.cat, length) == 2]
nrc.cat <- nrc.cat[lapply(nrc.cat, length) == 2]
nrc.cat <- lapply(nrc.cat, function(x) x[2])
nrc.cat <- lapply(nrc.cat, strsplit, split = "\\/")
nrc.cat <- lapply(nrc.cat, unlist)

for (i in seq_len(length(nrc.cat))){
    if (length(nrc.cat[[i]]) < max(unlist(lapply(nrc.cat, length)))){
        nrc.cat[[i]] <- c(nrc.cat[[i]], rep("", times = max(unlist(lapply(nrc.cat, length))) - length(nrc.cat[[i]])))
    }else{}
}

nrc.tab <- cbind(do.call(rbind, nrc.cat), nrc.url)
colnames(nrc.tab) <- c("Category", 
                       paste0("Sub-Category ", seq(1, ncol(nrc.tab)-2)),
                       "Resource")

```





## Manual

Data from [NRCS](https://www.nrcs.usda.gov/sites/default/files/2023-10/NRCS-CSAF-Mitigation-Activities-List.pdf)
were extracted manually using [tabula](https://tabula.technology/ "tabula").

```{r nrcs-pdf-tables, eval = FALSE}

csa.mit.tab <- read.csv("data/tabula-NRCS-CSAF-Mitigation-Activities.csv", header = FALSE)
csa.mit.head <- csa.mit.tab[grepl("Mitigation Categories", csa.mit.tab[, 1]), ]
csa.mit.head <- gsub("\\[.*?\\]", "", csa.mit.head)
csa.mit.head <- gsub("  ", " ", csa.mit.head)
colnames(csa.mit.tab) <- csa.mit.head
csa.mit.tab <- apply(csa.mit.tab, 2, gsub, pattern = "\\[.*?\\]", replace = "")
csa.mit.tab <- apply(csa.mit.tab, 2, gsub, pattern = "  ", replace = " ")

## Removing narrative crosswalk
csa.cwk <- csa.mit.tab[seq(grep("Waste Storage Structure", csa.mit.tab[, 2]), nrow(csa.mit.tab)), ]
colnames(csa.cwk)[4] <- "Narrative"
csa.mit.tab <-csa.mit.tab[-seq(grep("Waste Storage Structure", csa.mit.tab[, 2]), nrow(csa.mit.tab)), ]

## Generate urls for codes
get.codes <- function(x){
    x <- paste0(x, collapse = " ")
    x <- unlist(strsplit(x, split = " "))
    x <- x[grep("E[0-9][0-9][0-9][a-z,A-Z]", x)]
    return(x)
}

csa.mit.codes <- unlist(lapply(apply(csa.mit.tab, 1, get.codes), function(x) x[1]))
csa.mit.tab[, "Code"] <- csa.mit.codes
csa.mit.tab[!(grepl("E", csa.mit.tab[, "Code"])), "Code"] <- ""
csa.mit.url <- paste0("https://www.nrcs.usda.gov/sites/default/files/2022-11/", 
                      csa.mit.tab[, "Code"],
                      "_July_2022.pdf")
csa.mit.url <- gsub(" ", "-", csa.mit.url)
csa.mit.url[csa.mit.tab[, "Code"] == ""] <- ""
csa.mit.tab <- data.frame(csa.mit.tab, "URL" = csa.mit.url)

```

```{r old-db, eval = FALSE, echo = TRUE}

db.og <- as.data.frame(read_sheet("https://docs.google.com/spreadsheets/d/1AMlsLPDnwt01eEsBLdRe1hvhNSa3ofndcWX0gJ9xbUo/", sheet = "Original"))
db.jh <- as.data.frame(read_sheet("https://docs.google.com/spreadsheets/d/1AMlsLPDnwt01eEsBLdRe1hvhNSa3ofndcWX0gJ9xbUo/", sheet = "Jackson's Version"))

db.mg <- db_merge(db.og, db.jh) %>% 
    distinct

colnames(db.og)[colnames(db.og) == "Resources (Links)"] <- "Resource"
colnames(nrc.tab)[colnames(nrc.tab) == "Sub-Category 1"] <- "Sub-Category"

```



## Merge Data Streams


### Policy

```{r merge-policy, eval = TRUE, echo = TRUE}

policy <- read_sheet("https://docs.google.com/spreadsheets/d/1cWG_SH_gmi-S7E-59qLODGC82QOEIxI_QVRTu24sNzU/", sheet = 1)
policy <- as.data.frame(policy)
csa <- policy[, "CSA Goals"]

practice <- unique(nrcs.db[, "Practice"])
csa <- as.data.frame(policy)[, "CSA Goals"]

practice <- cbind(c("Pollinators", "Cover", "Rotation", "Crop", 
              "Filter", "Mulching", "Nutrient", "Digester", 
              "Critical", "Windbreak", "Silvopasture", "Buffer", 
              "Establishment", "Forest", "Pipeline", "Microirrigation", 
              "Sprinkler", "Pumping", "Lighting", "Wetland", 
              "Irrigation"), 
              c("Support Pollinators", "Conservation Cover", "Conservation Crop Rotation", 
                "Cover Crop", "Filter Strips", "Mulching", 
                "Nutrient Management", "Anaerobic Digester", "Critical Area Planting", 
                "Windbreak and Shelterbelt Establishment and Renovation", "Silvopasture", "Riparian Forest Buffer", 
                "Tree and Shrub Establishment", "Forest Stand Improvement", "Irrigation Pipeline", 
                "Irrigation System, Microirrigation", "Sprinkler System", "Pumping Plant", 
                "Energy Efficient Lighting System", "Wetland Restoration", "Irrigation Water Management"))

prac.miti <- nrcs.db[!(duplicated(nrcs.db[, "Practice"])), c("Practice", "Mitigation")]

pol.csa <- lapply(csa, function(x) sapply(x, detect.csa, practice = practice))

out <- list()
for (i in seq(1, nrow(policy))){
    x <- list()
    desc <- paste0(policy[i, "Guiding Policy"], ": ",
                   policy[i, "Description"], " See ", 
                   policy[i, "Verbiage (Location)"], ".")
    resc <- policy[i, "URL"]
    if (!(any(is.na(pol.csa[[i]])))){
        for (j in seq_along(pol.csa[[i]])){
            x[[j]] <- c(Practice = pol.csa[[i]][[j]], 
                        Description = desc,
                        Resource = resc
                        )
        }
    }else{
        x <- list(c(Practice = pol.csa[[i]][[1]], 
               Description = desc,
               Resource = resc
               ))
    }
    out[[i]] <- x
}

out <- lapply(out, function(x) do.call(rbind, x))
out <- do.call(rbind, out)

pol.db <- data.frame(Mitigation = prac.miti[match(out[, "Practice"], 
                                                  prac.miti[, "Practice"]), 
                                            "Mitigation"], out)
pol.db <- data.frame(Type = rep("Policy", nrow(pol.db)), pol.db)

```


### Supporting Resources


```{r merge-support, eval = TRUE, echo = TRUE}

support <- read_sheet("https://docs.google.com/spreadsheets/d/1ljbwFQuHo0Xn0u0ZD60YKZk8SdglJl5tpOgjSWbY9gA/", sheet = 1)
supp <- as.data.frame(support)
supp <- supp[!(is.na(supp[, "Description"])), ]

supp.db <- data.frame(supp[, c("Mitigation", "Practice")], 
                      Description = supp[, "Description"],
                      Resource = supp[, "Resources (Links)"])
supp.db <- data.frame(Type = rep("Support", nrow(supp.db)), 
                      supp.db)

```

# Saving Database

```{r nrcs-save, eval = TRUE, echo = TRUE}

hicsa.db <- nrcs.db

## Stop-gap fixes
## These need to be fixed on ingestion. Must be an issue with data structure. 
## Energy mis-matched info
hicsa.db[hicsa.db[, "Mitigation"] == "Energy, Combustion, and Electricity Efficiency", 
        "Practice"] <- c("Irrigation Pipeline",
                   "Irrigation System, Microirrigation",
                   "Sprinkler System",
                   "Pumping Plant", 
                   "Energy Efficient Lighting System")
## H4 is NA
hicsa.db[is.na(hicsa.db[, "Practice"]), "Mitigation"] <- "Conservation Efforts" 
hicsa.db[is.na(hicsa.db[, "Practice"]), "Practice"] <- "Support Pollinators" 
## Wetland not Energy Efficient Building Envelope
hicsa.db[hicsa.db[, "Mitigation"] == "Wetlands", "Practice"] <- "Wetland Restoration"
## Add Type
hicsa.db <- data.frame(Type = rep("Technical", nrow(hicsa.db)), hicsa.db)
## Merge NRCS, Policy and Support
hicsa.db <- rbind(hicsa.db, pol.db, supp.db)
hicsa.db <- hicsa.db[!(is.na(hicsa.db[, "Mitigation"])),]

## Export
saveRDS(hicsa.db, file = "./data/hi-csa-db.rds")

```


# Preview


```{r mit-table, eval = TRUE, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}

kable(head(hicsa.db[order(hicsa.db[, "Mitigation"]), ]), row.names = FALSE)

```
